defaults:
  - base_trainer
name: celeba_diffusion
platform: local
single_gpu: true
type: hugginface

# train: True #train from scratch
# eval: False #load ckpt.pt and evaluate FID and IS
test_session: False
qualitative_experiment: True
# Model ID hugginface
model_id: google/ddpm-ema-celebahq-256
log_root: /home/bastienvandelft/results #/mnt/scitas/bastien

# Dataset
datapath: /home/bastienvandelft/Datasets/CelebAMask-HQ/CelebA-HQ-img #/mnt/scitas/bastien/CelebAMask-HQ/CelebA-HQ-img #help='dataset path if downloaded
dataset: CELEBAHQ #help='dataset name')
exp_name_folder: 
lsun_category: None # one of ["bedroom","bridge","church_outdoor","classroom","conference_room","dining_room","kitchen","living_room","restaurant","tower",
corruptions_list: ['fog', 'contrast', 'spatter'] #,"frost","speckle_noise","elastic_transform",'impulse_noise', "shot_noise", "gaussian_noise",'masking_vline_random_color','masking_random_color',"brightness"] 
                  #["frost", "speckle_noise",'impulse_noise', "shot_noise", "gaussian_noise","jpeg_compression","pixelate",
                  #"fog","gaussian_blur","elastic_transform", "motion_blur","glass_blur","brightness", "saturate",
                  #'snow', 'masking_vline_random_color', 'spatter', 'contrast', 'masking_random_color']
split: all
use_val: false
num_workers: 4 #help='workers of Dataloader')
corruption:  #help=corruption type base on Imagenet-C
corruption_severity:  #help='corruption severity level 1-5'
random_flip: False #help='Whether to use random flip in training')
# lower_image_size: 
# original_img_size: []
img_size: 256 #help='image size')

# Inference
run_sdedit: true
run_all_epsilon: true
use_std_schedule: false
number_of_image: 1000
number_of_sample: 2
image_number: 
batch_size: 2 #help='batch size'
annealing: 1
annealing_cst: 0.75
normalize: false 
normalize_mean: false
clip_inputs: false
number_of_stds: 3
min_epsilon: 1e-5
max_epsilon: 1e-4
number_of_epsilons: 5
dynamic_thresholding_langevin: true
dynamic_thresholding_ddim: true
dynamic_threshol_ratio: 0.98
dynamic_threshold_max: 1.3
number_of_latents_corrected: 2
min_latent_space_update: 199
ode_range: [999, 1000, 1]
sde_range: [199,500,100]
# Logging & Sampling
number_of_steps: 1000
logdir: ${trainer.log_root}/logs/CelebaHQ #help='log directory')
base_dir: /home/bastienvandelft/Projects/ddpm
wandb_entity: bastienvd #help='wandb id to use')
ml_exp_name: ${trainer.dataset}_${trainer.lsun_category}_folder_${trainer.exp_name_folder} #help = 'name of the experience on wandb')


# UNet
# input_channel: 3
# kernel_size: 3
# ch: 128  #help='base channel of UNet')
# ch_mult: #"1,2,3,4" #[1, 2, 2, 4, 8] #help='channel multiplier')
# attention_resolutions: "32,16,8"
# # attn: [3,4] #help='add attention to these levels')
# num_res_blocks: 3 #help='# resblock in each level')
# dropout: 0. #help='dropout rate of resblock')
# num_heads: 4
# num_head_channels: -1 
# use_scale_shift_norm: False
# resblock_updown: False



# Gaussian Diffusion
# beta_schedule: linear
# num_timesteps: 1000 #help='total diffusion steps')
# beta_1: 1e-4 #help='start beta value')
# beta_T: 0.02 #help='end beta value')
# T: 1000 #help='total diffusion steps')

# mean_type: EPSILON #['PREVIOUS_X', 'START_X', 'EPSILON'], help='predict variable')
# var_type: FIXED_LARGE #['FIXED_LARGE', 'FIXED_SMALL', LEARNED_RANGE, LEARNED], help='variance type')
# schedule_sampler: uniform #"loss-second-moment" #"uniform"
# loss_type: MSE



# Training
# lr: 2e-4 #help='target learning rate')
# grad_clip:  #help="gradient norm clipping")
# total_steps: 1000000 #help='total training steps')
# warmup: 0 #help='learning rate warmup')
# batch_size: 8 #help='batch size')
# microbatch: 1
# num_workers: 4 #help='workers of Dataloader')
# ema_decay: 0.999 #help="ema decay rate")
# # ema_rate: 0.9999 #help="ema decay rate")
# weight_decay: 0.01
# fp16_scale_growth: 1e-3
# use_fp16: False
# offset_training: True

#Mixed Precision
# use_half_for_matmul: False
# use_half_for_conv: False
# use_half_precision: False

# Logging & Sampling
# number_of_steps: 1000
# logdir: /mnt/scitas/bastien/logs/CelebaHQ #help='log directory')
# wandb_entity: bastienvd #help='wandb id to use')
# # sample_size: 4 #"sampling size of images")
# # sample_step: 10000 #help='frequency of sampling')
# ml_exp_name: ${trainer.dataset}_${trainer.corruption}_x${trainer.img_size} #help = 'name of the experience on wandb')
# checkpointpath: /mnt/scitas/bastien/logs/Celeba/CELEBA_None_x64_2023-08-08_01-41/ckpt_170000.pt
# load_imagenet_256_ckpt: False
# imagenet_256_ckpt: /mnt/scitas/bastien/imagenet_diffusion/256x256_diffusion_uncond.pt
# timesteps_respacing: ddim100
# clip_denoised: False
# progress: False

# Evaluation
# save_step: 20000 #help='frequency of saving checkpoints, 0 to disable during training')
# eval_step: 0 #help='frequency of evaluating model, 0 to disable during training')
# num_images: 50000 #help='the number of generated images for evaluation')
# fid_use_torch: False #help='calculate IS and FID on gpu')
# fid_cache: ./stats/cifar10.train.npz #help='FID cache')