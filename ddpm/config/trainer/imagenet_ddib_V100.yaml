defaults:
  - base_trainer
name: imagenet_diffusion
platform: local
type: ddib

train: True #train from scratch
eval: False #load ckpt.pt and evaluate FID and IS
verbose: True

# Dataset
datapath: /mnt/scitas/datasets/imagenet #help='dataset path if downloaded
dataset: IMAGENET #help='dataset name')
split: all
corruption: [frost] #help=corruption type base on Imagenet-C
corruption_severity: [5] #help='corruption severity level 1-5'
random_flip: False #help='Whether to use random flip in training')

#original_img_size [1052,1914]
lower_image_size: 
original_img_size: [256,256]
img_size: 256 #help='image size')
center_crop: True

# UNet
input_channel: 3
kernel_size: 3
ch: 128  #help='base channel of UNet')
ch_mult:  #help='channel multiplier')
attention_resolutions: "32,16,8"
# attn: [3,4,5] #help='add attention to these levels')
num_res_blocks: 3 #help='# resblock in each level')
dropout: 0. #help='dropout rate of resblock')
num_heads: 4
num_head_channels: -1 
use_scale_shift_norm: False
resblock_updown: False

# Gaussian Diffusion
beta_schedule: linear
num_timesteps: 1000 #help='total diffusion steps')
# beta_1: 1e-4 #help='start beta value')
# beta_T: 0.02 #help='end beta value')
# T: ${trainer.num_timesteps} #help='total diffusion steps')
mean_type: EPSILON #['PREVIOUS_X', 'START_X', 'EPSILON'], help='predict variable')
var_type: LEARNED_RANGE #['FIXED_LARGE', 'FIXED_SMALL', LEARNED_RANGE, LEARNED], help='variance type')
schedule_sampler: "uniform" #"loss-second-moment" #"uniform"
loss_type: RESCALED_MSE

# Training
lr: 1e-7 #help='target learning rate')
grad_clip:  #help="gradient norm clipping")
total_steps: 1000000 #help='total training steps')
warmup:  #help='learning rate warmup')
batch_size: 8 #help='batch size')
microbatch: 1
num_workers: 4 #help='workers of Dataloader')
ema_decay: 0.9999 #help="ema decay rate")
# ema_rate: 0.9999 #help="ema decay rate")
weight_decay: 0.
fp16_scale_growth: 1e-3
use_fp16: True
# parallel: True #help='multi gpu training')
# unique_img: False #help='Train a model on a single image.')

#Mixed Precision
use_half_for_matmul: False
use_half_for_conv: False
use_half_precision: False

# Logging & Sampling
logdir: /mnt/scitas/bastien/logs/${trainer.dataset} #help='log directory')
wandb_entity: bastienvd #help='wandb id to use')
sample_size: 4 #"sampling size of images")
sample_step: 1000 #help='frequency of sampling')
ml_exp_name: Imagenet_Diffusion_cos_${trainer.corruption}_x${trainer.img_size} #help = 'name of the experience on wandb')
checkpointpath: #/mnt/scitas/bastien/logs/Audio/ckpt_350000.pt
load_imagenet_256_ckpt: True
imagenet_256_ckpt: /mnt/scitas/bastien/logs/ImageNet/256/256x256_diffusion_uncond.pt
timesteps_respacing: ddim50
clip_denoised: True
progress: False

# Evaluation
save_step: 10000 #help='frequency of saving checkpoints, 0 to disable during training')
eval_step: 0 #help='frequency of evaluating model, 0 to disable during training')
num_images: 500 #help='the number of generated images for evaluation')
fid_use_torch: False #help='calculate IS and FID on gpu')
fid_cache: ./stats/cifar10.train.npz #help='FID cache')

# Mode slurm
slurm:
  nodes: 2 # 2
  gpus_per_node: 2  # max 2
  cpus_per_task: 10
  mem: 72 # in GiB 48
  timeout: 24 # hours
  partition: gpu
  qos: gpu
  account: vita # optional
  reservation:
